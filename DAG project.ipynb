{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOt6JrTEv6ScL9gKiEReQyQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["etl_pipeline/\n","├── dags/\n","│   └── etl_dag.py\n","├── plugins/\n","│   └── custom_transform_operator.py\n","├── utils/\n","│   └── transformations.py\n"],"metadata":{"id":"aXSPDyx_6nCZ"}},{"cell_type":"code","source":["#Dependencies\n","!pip install apache-airflow"],"metadata":{"id":"uWHWf0hP8I3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. etl_dag.py: Define the Airflow DAG\n","\n","# dags/etl_dag.py\n","\n","from airflow import DAG\n","from airflow.operators.python import PythonOperator\n","from datetime import datetime, timedelta\n","from plugins.custom_transform_operator import CustomTransformOperator\n","from utils.transformations import standardize_and_clean_data\n","\n","\n","def extract_from_api(**kwargs):\n","    import requests\n","    response = requests.get('https://api.example.com/data')\n","    data = response.json()\n","    kwargs['ti'].xcom_push(key='api_data', value=data)\n","\n","\n","def extract_from_s3(**kwargs):\n","    import boto3\n","    import json\n","    s3 = boto3.client('s3')\n","    response = s3.get_object(Bucket='my-bucket', Key='data/file.json')\n","    data = json.loads(response['Body'].read())\n","    kwargs['ti'].xcom_push(key='s3_data', value=data)\n","\n","\n","def load_to_warehouse(**kwargs):\n","    import pandas as pd\n","    from sqlalchemy import create_engine\n","\n","    transformed_data = kwargs['ti'].xcom_pull(key='final_data')\n","    df = pd.DataFrame(transformed_data)\n","\n","    engine = create_engine('postgresql://user:pass@host:5432/warehouse')\n","    df.to_sql('target_table', con=engine, if_exists='replace', index=False)\n","\n","\n","default_args = {\n","    'owner': 'airflow',\n","    'start_date': datetime(2023, 1, 1),\n","    'retries': 1,\n","    'retry_delay': timedelta(minutes=5)\n","}\n","\n","with DAG(\n","    dag_id='modular_etl_pipeline',\n","    default_args=default_args,\n","    schedule_interval='@daily',\n","    catchup=False,\n","    tags=['etl', 'airflow', 'data-pipeline']\n",") as dag:\n","\n","    extract_api = PythonOperator(\n","        task_id='extract_from_api',\n","        python_callable=extract_from_api,\n","        provide_context=True\n","    )\n","\n","    extract_s3 = PythonOperator(\n","        task_id='extract_from_s3',\n","        python_callable=extract_from_s3,\n","        provide_context=True\n","    )\n","\n","    transform = CustomTransformOperator(\n","        task_id='transform_data'\n","    )\n","\n","    clean_data = PythonOperator(\n","        task_id='standardize_and_clean',\n","        python_callable=standardize_and_clean_data,\n","        provide_context=True\n","    )\n","\n","    load = PythonOperator(\n","        task_id='load_to_warehouse',\n","        python_callable=load_to_warehouse,\n","        provide_context=True\n","    )\n","\n","    [extract_api, extract_s3] >> transform >> clean_data >> load\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"KBVmOYwk6rnx","executionInfo":{"status":"error","timestamp":1745081470493,"user_tz":-180,"elapsed":41,"user":{"displayName":"Francis Oundo","userId":"07764030066658731234"}},"outputId":"13cae4b7-ea63-4f3f-f836-2b68a2ed06ef"},"execution_count":7,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'plugins'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-5e9fa128a665>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPythonOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_transform_operator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomTransformOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstandardize_and_clean_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plugins'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["## Custom Transformation Operator\n","\n","# plugins/custom_transform_operator.py\n","\n","from airflow.models import BaseOperator\n","from airflow.utils.decorators import apply_defaults\n","\n","\n","class CustomTransformOperator(BaseOperator):\n","    @apply_defaults\n","    def __init__(self, *args, **kwargs):\n","        super(CustomTransformOperator, self).__init__(*args, **kwargs)\n","\n","    def execute(self, context):\n","        api_data = context['ti'].xcom_pull(key='api_data')\n","        s3_data = context['ti'].xcom_pull(key='s3_data')\n","\n","        # Combine and transform\n","        combined = api_data + s3_data\n","        filtered = [record for record in combined if record.get('status') == 'active']\n","\n","        totals = {}\n","        for item in filtered:\n","            category = item['category']\n","            totals[category] = totals.get(category, 0) + 1\n","\n","        context['ti'].xcom_push(key='transformed_data', value=totals)\n"],"metadata":{"id":"broeVHa68J84","executionInfo":{"status":"ok","timestamp":1745081462858,"user_tz":-180,"elapsed":57,"user":{"displayName":"Francis Oundo","userId":"07764030066658731234"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["##Standardize and Clean Function\n","\n","# utils/transformations.py\n","\n","def standardize_and_clean_data(**kwargs):\n","    import pandas as pd\n","\n","    transformed_data = kwargs['ti'].xcom_pull(key='transformed_data')\n","    df = pd.DataFrame([\n","        {'category': key, 'count': value}\n","        for key, value in transformed_data.items()\n","    ])\n","\n","    df['category'] = df['category'].str.lower().str.strip()\n","    df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0)\n","\n","    kwargs['ti'].xcom_push(key='final_data', value=df.to_dict(orient='records'))\n"],"metadata":{"id":"g4UtNgBi9fzS"},"execution_count":null,"outputs":[]}]}